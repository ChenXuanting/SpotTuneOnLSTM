{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GsmM1pnJ_ooZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theo\\anaconda3\\envs\\newenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dR9tTOIfXKBE"
   },
   "source": [
    "**Source Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jJPiS48oZv_D"
   },
   "outputs": [],
   "source": [
    "classes = [\"a09\", \"a13\", \"a14\", \"a15\", \"a07\", \"a03\"]\n",
    "persons = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\", \"p8\"]\n",
    "x_train_source = []\n",
    "y_train_source = []\n",
    "for cl in range(6):\n",
    "  for p in persons:\n",
    "    for s in range(60):\n",
    "      if s+1 < 10:\n",
    "        link = \"C:/Users/Theo/ECE685/project/data/\"+classes[cl]+\"/\"+p+\"/s0\"+str(s+1)+\".txt\"\n",
    "      else:\n",
    "        link = \"C:/Users/Theo/ECE685/project/data/\"+classes[cl]+\"/\"+p+\"/s\"+str(s+1)+\".txt\"\n",
    "      f = open(link, \"r\")\n",
    "      data_point = []\n",
    "      for x in f:\n",
    "        ax = [float(i) for i in x.split(',')[:6]]\n",
    "        data_point.append(ax)\n",
    "      x_train_source.append(data_point)\n",
    "      y_train_source.append(cl+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0ou_o3OlftrT"
   },
   "outputs": [],
   "source": [
    "x_train_source = torch.from_numpy(np.array(x_train_source)).reshape(2880,125,6).type(torch.FloatTensor)\n",
    "y_train_source = torch.from_numpy(np.array(y_train_source)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWhmYSqaXMl6"
   },
   "source": [
    "**Target Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VR7rVp0S5NJo"
   },
   "outputs": [],
   "source": [
    "bax = torch.from_numpy(np.loadtxt(\"body_acc_x_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
    "bay = torch.from_numpy(np.loadtxt(\"body_acc_y_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
    "baz = torch.from_numpy(np.loadtxt(\"body_acc_z_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
    "bgx = torch.from_numpy(np.loadtxt(\"body_gyro_x_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
    "bgy = torch.from_numpy(np.loadtxt(\"body_gyro_y_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
    "bgz = torch.from_numpy(np.loadtxt(\"body_gyro_z_train.txt\")[:,:125]).reshape(7352,125,1).type(torch.FloatTensor)\n",
    "x_train = torch.cat((bax, bay, baz, bgx, bgy, bgz), 2)\n",
    "y_train = torch.from_numpy(np.loadtxt(\"y_train.txt\")).type(torch.FloatTensor) \n",
    "\n",
    "x_train = x_train[int(len(x_train)/2):]\n",
    "y_train = y_train[int(len(y_train)/2):]\n",
    "\n",
    "bax_test = torch.from_numpy(np.loadtxt(\"body_acc_x_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
    "bay_test = torch.from_numpy(np.loadtxt(\"body_acc_y_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
    "baz_test = torch.from_numpy(np.loadtxt(\"body_acc_z_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
    "bgx_test = torch.from_numpy(np.loadtxt(\"body_gyro_x_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
    "bgy_test = torch.from_numpy(np.loadtxt(\"body_gyro_y_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
    "bgz_test = torch.from_numpy(np.loadtxt(\"body_gyro_z_test.txt\")[:,:125]).reshape(2947,125,1).type(torch.FloatTensor)\n",
    "x_test = torch.cat((bax_test, bay_test, baz_test, bgx_test, bgy_test, bgz_test), 2)\n",
    "y_test = torch.from_numpy(np.loadtxt(\"y_test.txt\")).type(torch.FloatTensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i6dM-QgAAHNt"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  def __init__(self, n_input, n_hidden,\n",
    "                 n_classes, drop_prob):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_input = n_input\n",
    "\n",
    "        self.lstm1 = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        #self.batch = nn.BatchNorm2d()\n",
    "        self.drop1 = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(n_hidden, 1)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "        out, (hn,cn) = self.lstm1(x)\n",
    "        out, (hn,cn) = self.lstm2(out)\n",
    "        out, (hn,cn) = self.lstm3(out)\n",
    "        out = self.fc1(self.drop1(out))\n",
    "        #out = self.fc2(self.drop1(out[:,:,0]))\n",
    "        #out = self.fc1(out)\n",
    "        out = self.fc2(out[:,:,0])\n",
    "        out = self.activation(out)\n",
    "        return out #(torch.argmax(out, dim = 1) + 1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "        ''' Initialize hidden state'''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        # if (train_on_gpu):\n",
    "        if (torch.cuda.is_available() ):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MQrexU_tOnz6"
   },
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "  preds = torch.argmax(output, dim = 1)\n",
    "  c = 0\n",
    "  for i in range(len(preds)):\n",
    "    if preds[i] == target[i]:\n",
    "      c += 1\n",
    "  return c/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-7sP4Z5ZMvRH"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, X, Y, num_epochs, num_classes, batch_size):\n",
    "  criterion = nn.NLLLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "  #data = []\n",
    "  #for i in range(len(X)):\n",
    "  #  data.append([X[i], Y[i]])\n",
    "  dataset = torch.utils.data.TensorDataset(torch.Tensor(np.array(X)), torch.Tensor(np.array(Y)))\n",
    "  train_set_size = int(len(dataset)*0.8)\n",
    "  val_set_size = len(dataset) - train_set_size\n",
    "  train_set, val_set = torch.utils.data.random_split(dataset, [train_set_size, val_set_size])\n",
    "  trainloader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=100)\n",
    "  valloader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=100)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    step_loss = 0\n",
    "    train_acc = []\n",
    "    model.train()\n",
    "    for step, (x, y) in enumerate(trainloader):\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model.forward(x.float())\n",
    "      loss = criterion(torch.log(outputs), y.long()-1)\n",
    "      train_acc.append(accuracy(outputs,y-1)*len(x))\n",
    "      #loss.requires_grad = True\n",
    "      loss.backward() #calculates the loss of the loss function\n",
    "      optimizer.step()\n",
    "      step_loss += loss\n",
    "    print(\"Training Loss: \",step_loss.item())\n",
    "    print(\"Training accuracy: \",np.sum(train_acc)/train_set_size)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = []\n",
    "    for step, (x, y) in enumerate(valloader):\n",
    "      outputs = model.forward(x.float())\n",
    "      loss = criterion(torch.log(outputs), y.long()-1)\n",
    "      val_acc.append(accuracy(outputs,y-1)*len(x))\n",
    "      #loss.requires_grad = True\n",
    "      val_loss += loss\n",
    "    print(\"Validation Loss: \",val_loss.item())\n",
    "    print(\"Validation accuracy: \",np.sum(val_acc)/val_set_size)\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqSwlknl7XkS",
    "outputId": "bf82dfd3-026b-4893-ce8e-dd8a285d1dfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theo\\anaconda3\\envs\\newenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  40.48585891723633\n",
      "Training accuracy:  0.24869791666666666\n",
      "Validation Loss:  8.416284561157227\n",
      "Validation accuracy:  0.3541666666666667\n",
      "Training Loss:  33.47208023071289\n",
      "Training accuracy:  0.3263888888888889\n",
      "Validation Loss:  7.6295695304870605\n",
      "Validation accuracy:  0.3888888888888889\n",
      "Training Loss:  31.206132888793945\n",
      "Training accuracy:  0.3363715277777778\n",
      "Validation Loss:  7.587491989135742\n",
      "Validation accuracy:  0.3385416666666667\n",
      "Training Loss:  30.2963924407959\n",
      "Training accuracy:  0.3372395833333333\n",
      "Validation Loss:  8.054112434387207\n",
      "Validation accuracy:  0.3680555555555556\n",
      "Training Loss:  30.48200035095215\n",
      "Training accuracy:  0.3519965277777778\n",
      "Validation Loss:  7.091183662414551\n",
      "Validation accuracy:  0.4045138888888889\n",
      "Training Loss:  28.955646514892578\n",
      "Training accuracy:  0.40625\n",
      "Validation Loss:  6.777156829833984\n",
      "Validation accuracy:  0.5590277777777778\n",
      "Training Loss:  28.13119888305664\n",
      "Training accuracy:  0.4704861111111111\n",
      "Validation Loss:  6.349762916564941\n",
      "Validation accuracy:  0.5572916666666666\n",
      "Training Loss:  24.711462020874023\n",
      "Training accuracy:  0.4895833333333333\n",
      "Validation Loss:  5.2995100021362305\n",
      "Validation accuracy:  0.5138888888888888\n",
      "Training Loss:  21.359315872192383\n",
      "Training accuracy:  0.5325520833333334\n",
      "Validation Loss:  5.106808662414551\n",
      "Validation accuracy:  0.5798611111111112\n",
      "Training Loss:  21.265377044677734\n",
      "Training accuracy:  0.5555555555555556\n",
      "Validation Loss:  4.414554119110107\n",
      "Validation accuracy:  0.6354166666666666\n",
      "Training Loss:  19.061866760253906\n",
      "Training accuracy:  0.6154513888888888\n",
      "Validation Loss:  3.899428367614746\n",
      "Validation accuracy:  0.6649305555555556\n",
      "Training Loss:  16.185157775878906\n",
      "Training accuracy:  0.6692708333333334\n",
      "Validation Loss:  3.4247353076934814\n",
      "Validation accuracy:  0.71875\n",
      "Training Loss:  13.452162742614746\n",
      "Training accuracy:  0.7374131944444444\n",
      "Validation Loss:  3.6140666007995605\n",
      "Validation accuracy:  0.7430555555555556\n",
      "Training Loss:  12.790095329284668\n",
      "Training accuracy:  0.7634548611111112\n",
      "Validation Loss:  3.1743760108947754\n",
      "Validation accuracy:  0.7395833333333334\n",
      "Training Loss:  11.819123268127441\n",
      "Training accuracy:  0.7621527777777778\n",
      "Validation Loss:  2.6388916969299316\n",
      "Validation accuracy:  0.8107638888888888\n",
      "Training Loss:  12.21815013885498\n",
      "Training accuracy:  0.7756076388888888\n",
      "Validation Loss:  3.3966989517211914\n",
      "Validation accuracy:  0.703125\n",
      "Training Loss:  10.825928688049316\n",
      "Training accuracy:  0.7981770833333334\n",
      "Validation Loss:  2.335906982421875\n",
      "Validation accuracy:  0.8333333333333334\n",
      "Training Loss:  9.372079849243164\n",
      "Training accuracy:  0.8302951388888888\n",
      "Validation Loss:  2.0862951278686523\n",
      "Validation accuracy:  0.84375\n",
      "Training Loss:  7.577408790588379\n",
      "Training accuracy:  0.8680555555555556\n",
      "Validation Loss:  1.640886664390564\n",
      "Validation accuracy:  0.8680555555555556\n",
      "Training Loss:  7.221505165100098\n",
      "Training accuracy:  0.859375\n",
      "Validation Loss:  1.8172041177749634\n",
      "Validation accuracy:  0.8663194444444444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model = train(x_train_source, y_train_source, 20, 6, 100)\n",
    "pre_trained_model = LSTM(6,125,6,.3)\n",
    "pre_trained_model = train(pre_trained_model, x_train_source, y_train_source, 20, 6, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dn-3KzP6ifYe"
   },
   "source": [
    "Spottune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NMZx2TdgGhgf"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "  def __init__(self, n_input, n_hidden, n_output, drop_prob):\n",
    "    super(Agent, self).__init__()\n",
    "\n",
    "    self.n_input = n_input\n",
    "    self.n_output = n_output\n",
    "    self.n_hidden = n_hidden\n",
    "    self.drop_prob = drop_prob\n",
    "\n",
    "    self.lstm1 = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "    self.lstm2 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "    #self.batch = nn.BatchNorm2d()\n",
    "    self.drop1 = nn.Dropout(drop_prob)\n",
    "    self.fc1 = nn.Linear(n_hidden, 1)\n",
    "    self.fc2 = nn.Linear(n_hidden, n_output)\n",
    "    self.activation = nn.Softmax(dim=1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "        out, (hn,cn) = self.lstm1(x)\n",
    "        out, (hn, cn) = self.lstm2(out)\n",
    "        out = self.fc1(self.drop1(out))\n",
    "        #out = self.fc2(self.drop1(out[:,:,0]))\n",
    "        #out = self.fc1(out)\n",
    "        out = self.fc2(out[:,:,0])\n",
    "        out = self.activation(out)\n",
    "        return out #(torch.argmax(out, dim = 1) + 1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "        ''' Initialize hidden state'''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        # if (train_on_gpu):\n",
    "        if (torch.cuda.is_available() ):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cXnuENJjigu2"
   },
   "outputs": [],
   "source": [
    "class SpottuneLSTM(nn.Module):\n",
    "  def __init__(self, n_input, n_hidden,\n",
    "                 n_classes, drop_prob):\n",
    "        super(SpottuneLSTM, self).__init__()\n",
    "\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_input = n_input\n",
    "\n",
    "        self.lstm1 = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "\n",
    "        self.lstm1_frozen = nn.LSTM(n_input, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        self.lstm2_frozen = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "        self.lstm3_frozen = nn.LSTM(n_hidden, n_hidden, 1, dropout=self.drop_prob, batch_first=True)\n",
    "\n",
    "\n",
    "        #self.batch = nn.BatchNorm2d()\n",
    "        self.drop1 = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(n_hidden, 1)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_classes)\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "  def lstm_output(self, lstm, lstm_frozen, policy, x, n_input):\n",
    "    for i in range(len(policy)):\n",
    "      ax = torch.reshape(x[i], (1, 125, n_input))\n",
    "      if policy[i] == 0.:\n",
    "        out_i, (_, _) = lstm_frozen(ax)\n",
    "      else:\n",
    "        out_i, (_, _) = lstm(ax)\n",
    "      if i == 0:\n",
    "        out = out_i\n",
    "      else:\n",
    "        out = torch.cat((out, out_i), 0)\n",
    "    return out\n",
    "  def forward(self, x, policy = None):\n",
    "        out = self.lstm_output(self.lstm1, self.lstm1_frozen, policy[:, 0], x, self.n_input)\n",
    "        #out, (hn,cn) = policy[:,0]*self.lstm1(x)+(1-policy[:,0])*self.lstm1_frozen(x)\n",
    "        #out, (hn,cn) = policy[:,1]*self.lstm2(x)+(1-policy[:,1])*self.lstm2_frozen(x)\n",
    "        out = self.lstm_output(self.lstm2, self.lstm2_frozen, policy[:, 1], out, self.n_hidden)\n",
    "        #out, (hn,cn) = policy[:,2]*self.lstm3(x)+(1-policy[:,2])*self.lstm3_frozen(x)\n",
    "        out = self.lstm_output(self.lstm3, self.lstm3_frozen, policy[:, 2], out, self.n_hidden)\n",
    "        out = self.fc1(self.drop1(out))\n",
    "        #out = self.fc2(self.drop1(out[:,:,0]))\n",
    "        #out = self.fc1(out)\n",
    "        out = self.fc2(out[:,:,0])\n",
    "        out = self.activation(out)\n",
    "        return out #(torch.argmax(out, dim = 1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "HnKtftR7io4G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  147.43612670898438\n",
      "Training Accuracy:  0.1683673469387755\n",
      "Validation Loss:  21.207746505737305\n",
      "Validation Accuracy:  0.1644021739130435\n",
      "Training Loss:  60.96635437011719\n",
      "Training Accuracy:  0.21768707482993196\n",
      "Validation Loss:  13.89486312866211\n",
      "Validation Accuracy:  0.26358695652173914\n",
      "Training Loss:  50.20680618286133\n",
      "Training Accuracy:  0.2578231292517007\n",
      "Validation Loss:  12.968700408935547\n",
      "Validation Accuracy:  0.24048913043478262\n",
      "Training Loss:  45.84107971191406\n",
      "Training Accuracy:  0.295578231292517\n",
      "Validation Loss:  11.377649307250977\n",
      "Validation Accuracy:  0.3138586956521739\n",
      "Training Loss:  42.45390319824219\n",
      "Training Accuracy:  0.3217687074829932\n",
      "Validation Loss:  11.117589950561523\n",
      "Validation Accuracy:  0.31657608695652173\n",
      "Training Loss:  41.06414794921875\n",
      "Training Accuracy:  0.33163265306122447\n",
      "Validation Loss:  10.664321899414062\n",
      "Validation Accuracy:  0.327445652173913\n",
      "Training Loss:  39.267181396484375\n",
      "Training Accuracy:  0.33945578231292517\n",
      "Validation Loss:  9.906705856323242\n",
      "Validation Accuracy:  0.34782608695652173\n",
      "Training Loss:  44.06346893310547\n",
      "Training Accuracy:  0.3\n",
      "Validation Loss:  11.146697998046875\n",
      "Validation Accuracy:  0.29483695652173914\n",
      "Training Loss:  40.81328582763672\n",
      "Training Accuracy:  0.30374149659863947\n",
      "Validation Loss:  10.555903434753418\n",
      "Validation Accuracy:  0.29891304347826086\n",
      "Training Loss:  37.81337356567383\n",
      "Training Accuracy:  0.3442176870748299\n",
      "Validation Loss:  10.048463821411133\n",
      "Validation Accuracy:  0.35597826086956524\n"
     ]
    }
   ],
   "source": [
    "##It's working, it ust needed time and I wanted to go to sleep. Goodnight!!\n",
    "\n",
    "spottunemodel = SpottuneLSTM(6,125,6,.3)\n",
    "spottunemodel.lstm1_frozen.weight_ih_l0.requires_grad = False\n",
    "spottunemodel.lstm1_frozen.weight_hh_l0.requires_grad = False\n",
    "spottunemodel.lstm1_frozen.bias_ih_l0.requires_grad = False\n",
    "spottunemodel.lstm1_frozen.bias_hh_l0.requires_grad = False\n",
    "\n",
    "spottunemodel.lstm2_frozen.weight_ih_l0.requires_grad = False\n",
    "spottunemodel.lstm2_frozen.weight_hh_l0.requires_grad = False\n",
    "spottunemodel.lstm2_frozen.bias_ih_l0.requires_grad = False\n",
    "spottunemodel.lstm2_frozen.bias_hh_l0.requires_grad = False\n",
    "\n",
    "spottunemodel.lstm3_frozen.weight_ih_l0.requires_grad = False\n",
    "spottunemodel.lstm3_frozen.weight_hh_l0.requires_grad = False\n",
    "spottunemodel.lstm3_frozen.bias_ih_l0.requires_grad = False\n",
    "spottunemodel.lstm3_frozen.bias_hh_l0.requires_grad = False\n",
    "states = pre_trained_model.state_dict()\n",
    "states[\"lstm1_frozen.weight_ih_l0\"] = states[\"lstm1.weight_ih_l0\"]\n",
    "states[\"lstm1_frozen.weight_hh_l0\"] = states[\"lstm1.weight_hh_l0\"]\n",
    "states[\"lstm1_frozen.bias_ih_l0\"] = states[\"lstm1.bias_ih_l0\"]\n",
    "states[\"lstm1_frozen.bias_hh_l0\"] = states[\"lstm1.bias_hh_l0\"]\n",
    "states[\"lstm2_frozen.weight_ih_l0\"] = states[\"lstm2.weight_ih_l0\"]\n",
    "states[\"lstm2_frozen.weight_hh_l0\"] = states[\"lstm2.weight_hh_l0\"]\n",
    "states[\"lstm2_frozen.bias_ih_l0\"] = states[\"lstm2.bias_ih_l0\"]\n",
    "states[\"lstm2_frozen.bias_hh_l0\"] = states[\"lstm2.bias_hh_l0\"]\n",
    "states[\"lstm3_frozen.weight_ih_l0\"] = states[\"lstm3.weight_ih_l0\"]\n",
    "states[\"lstm3_frozen.weight_hh_l0\"] = states[\"lstm3.weight_hh_l0\"]\n",
    "states[\"lstm3_frozen.bias_ih_l0\"] = states[\"lstm3.bias_ih_l0\"]\n",
    "states[\"lstm3_frozen.bias_hh_l0\"] = states[\"lstm3.bias_hh_l0\"]\n",
    "spottunemodel.load_state_dict(states)\n",
    "\n",
    "def spottunetrain(model, agent, X, Y, num_epochs, num_classes, batch_size):\n",
    "  criterion = nn.NLLLoss()\n",
    "  optimizer_model = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "  optimizer_agent = torch.optim.Adam(agent.parameters(), lr=.001)\n",
    "  dataset = torch.utils.data.TensorDataset(torch.Tensor(np.array(X)), torch.Tensor(np.array(Y)))\n",
    "  train_set_size = int(len(dataset)*0.8)\n",
    "  val_set_size = len(dataset) - train_set_size\n",
    "  train_set, val_set = torch.utils.data.random_split(dataset, [train_set_size, val_set_size])\n",
    "  trainloader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=100)\n",
    "  valloader = torch.utils.data.DataLoader(val_set, shuffle=True, batch_size=100)\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    step_loss = 0\n",
    "    train_acc = []\n",
    "    for step, (x, y) in enumerate(trainloader):\n",
    "      optimizer_model.zero_grad()\n",
    "      optimizer_agent.zero_grad()\n",
    "      policy = agent(x)\n",
    "      action = F.gumbel_softmax(policy, tau=1, hard=True)\n",
    "      outputs = model.forward(x.float(), action)\n",
    "      loss = criterion(torch.log(outputs), y.long()-1)\n",
    "      train_acc.append(accuracy(outputs,y-1)*len(x))\n",
    "      #loss.requires_grad = True\n",
    "      loss.backward() #calculates the loss of the loss function\n",
    "      if action[0][0] == 0:\n",
    "        if spottunemodel.lstm1.weight_ih_l0.grad is not None:\n",
    "          spottunemodel.lstm1.weight_ih_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm1.weight_hh_l0.grad is not None:\n",
    "          spottunemodel.lstm1.weight_hh_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm1.bias_ih_l0.grad is not None:\n",
    "          spottunemodel.lstm1.bias_ih_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm1.bias_hh_l0.grad is not None:\n",
    "          spottunemodel.lstm1.bias_hh_l0.grad.data.zero_()\n",
    "      if action[0][1] == 0:\n",
    "        if spottunemodel.lstm2.weight_ih_l0.grad is not None:\n",
    "          spottunemodel.lstm2.weight_ih_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm2.weight_hh_l0.grad is not None:\n",
    "          spottunemodel.lstm2.weight_hh_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm2.bias_ih_l0.grad is not None:\n",
    "          spottunemodel.lstm2.bias_ih_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm2.bias_hh_l0.grad is not None:\n",
    "          spottunemodel.lstm2.bias_hh_l0.grad.data.zero_()\n",
    "      if action[0][2] == 0:\n",
    "        if spottunemodel.lstm3.weight_ih_l0.grad is not None:\n",
    "          spottunemodel.lstm3.weight_ih_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm3.weight_hh_l0.grad is not None:\n",
    "          spottunemodel.lstm3.weight_hh_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm3.bias_ih_l0.grad is not None:\n",
    "          spottunemodel.lstm3.bias_ih_l0.grad.data.zero_()\n",
    "        if spottunemodel.lstm3.bias_hh_l0.grad is not None:\n",
    "          spottunemodel.lstm3.bias_hh_l0.grad.data.zero_()\n",
    "      optimizer_model.step()\n",
    "      optimizer_agent.step()\n",
    "      step_loss += loss\n",
    "    print(\"Training Loss: \",step_loss.item())\n",
    "    print(\"Training Accuracy: \",np.sum(train_acc)/train_set_size)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_acc = []\n",
    "    for step, (x, y) in enumerate(valloader):\n",
    "      policy = agent(x)\n",
    "      action = F.gumbel_softmax(policy, tau=1, hard=True)\n",
    "      outputs = model.forward(x.float(), action)\n",
    "      loss = criterion(torch.log(outputs), y.long()-1)\n",
    "      val_acc.append(accuracy(outputs,y-1)*len(x))\n",
    "      val_loss += loss\n",
    "    print(\"Validation Loss: \",val_loss.item())\n",
    "    print(\"Validation Accuracy: \",np.sum(val_acc)/val_set_size)\n",
    "  return model\n",
    "\n",
    "agent = Agent(6, 125, 3, 0.3)\n",
    "#spottunemodel = spottunetrain(spottunemodel, agent, x_train, y_train, 20, 6, 100)\n",
    "spottunemodel = spottunetrain(spottunemodel, agent, x_train, y_train, 10, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Zy3GIoF78V0H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  1.4175454378128052\n",
      "Test Accuracy:  0.31964709874448594\n"
     ]
    }
   ],
   "source": [
    "testset = torch.utils.data.TensorDataset(torch.Tensor(np.array(x_test)), torch.Tensor(np.array(y_test)))\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, batch_size=len(testset))\n",
    "criterion = nn.NLLLoss()\n",
    "spottunemodel.eval()\n",
    "test_loss = 0\n",
    "test_acc = []\n",
    "for step, (x, y) in enumerate(testloader):\n",
    "  policy = agent(x)\n",
    "  action = F.gumbel_softmax(policy, tau=1, hard=True)\n",
    "  outputs = spottunemodel.forward(x.float(), action)\n",
    "  loss = criterion(torch.log(outputs), y.long()-1)\n",
    "  test_acc.append(accuracy(outputs,y-1)*len(x))\n",
    "  test_loss += loss\n",
    "print(\"Test Loss: \",test_loss.item())\n",
    "print(\"Test Accuracy: \",np.sum(test_acc)/len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RhoUNsL4-Ms"
   },
   "source": [
    "**Benchmark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_P80hpx49-T",
    "outputId": "f1baaa94-23ce-4deb-b132-ba5e0001d701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  53.328147888183594\n",
      "Training accuracy:  0.19931972789115646\n",
      "Validation Loss:  13.563400268554688\n",
      "Validation accuracy:  0.3029891304347826\n",
      "Training Loss:  39.78975296020508\n",
      "Training accuracy:  0.33095238095238094\n",
      "Validation Loss:  9.353944778442383\n",
      "Validation accuracy:  0.32608695652173914\n",
      "Training Loss:  34.06383514404297\n",
      "Training accuracy:  0.3608843537414966\n",
      "Validation Loss:  9.096637725830078\n",
      "Validation accuracy:  0.33695652173913043\n",
      "Training Loss:  33.830284118652344\n",
      "Training accuracy:  0.34183673469387754\n",
      "Validation Loss:  8.9976806640625\n",
      "Validation accuracy:  0.3342391304347826\n",
      "Training Loss:  33.53144073486328\n",
      "Training accuracy:  0.34965986394557824\n",
      "Validation Loss:  9.065139770507812\n",
      "Validation accuracy:  0.33559782608695654\n",
      "Training Loss:  33.273189544677734\n",
      "Training accuracy:  0.36156462585034016\n",
      "Validation Loss:  8.917169570922852\n",
      "Validation accuracy:  0.35054347826086957\n",
      "Training Loss:  33.17144012451172\n",
      "Training accuracy:  0.35714285714285715\n",
      "Validation Loss:  9.058509826660156\n",
      "Validation accuracy:  0.34103260869565216\n",
      "Training Loss:  33.1656494140625\n",
      "Training accuracy:  0.3687074829931973\n",
      "Validation Loss:  9.015589714050293\n",
      "Validation accuracy:  0.3328804347826087\n",
      "Training Loss:  32.88458251953125\n",
      "Training accuracy:  0.37517006802721087\n",
      "Validation Loss:  8.92916202545166\n",
      "Validation accuracy:  0.30978260869565216\n",
      "Training Loss:  32.8752555847168\n",
      "Training accuracy:  0.3656462585034014\n",
      "Validation Loss:  8.900971412658691\n",
      "Validation accuracy:  0.3125\n"
     ]
    }
   ],
   "source": [
    "benchmark_model = LSTM(6,125,6,.3)\n",
    "benchmark_model = train(benchmark_model, x_train, y_train, 10, 6, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUyv4gOR5h_E",
    "outputId": "8458d366-acb0-41ec-e5dc-5ad556915091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss:  1.146884560585022\n",
      "Test Accuracy:  0.34781133355955207\n"
     ]
    }
   ],
   "source": [
    "testset = torch.utils.data.TensorDataset(torch.Tensor(np.array(x_test)), torch.Tensor(np.array(y_test)))\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, batch_size=len(testset))\n",
    "criterion = nn.NLLLoss()\n",
    "benchmark_model.eval()\n",
    "test_loss = 0\n",
    "test_acc = []\n",
    "for step, (x, y) in enumerate(testloader):\n",
    "  outputs = benchmark_model.forward(x.float())\n",
    "  loss = criterion(torch.log(outputs), y.long()-1)\n",
    "  test_acc.append(accuracy(outputs,y-1)*len(x))\n",
    "  test_loss += loss\n",
    "print(\"Test Loss: \",test_loss.item())\n",
    "print(\"Test Accuracy: \",np.sum(test_acc)/len(x_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
